hydra:
  job:
    chdir: false
output_file: ???                 # refer to `bm25_retriever.yaml` for explanation
num_candidates: 50
num_ice: 8
task_name: ???
qa_model_name: ???               # inference model name

batch_size: 64                   # the batch size when running encoding
faiss_index: ???                 # if file exists, the encoded index data will be directly loaded
index_dict: ???                  # if file exists, the dict {index: encoded data} will be directly loaded
model_name: 'bert-base-uncased'  # model used to encode 'field' for each index instance

# sentence-bert setting
sb_retriever: true
encode_model: 'all-MiniLM-L6-v2'

# DPP-related arguments
dpp_search: false                # whether to employ DPP search that considers diversity between ices
dpp_topk: 100                    # the number to retrieve by TopK in the first stage of DPP
mode: cand_random                # pure_random, cand_random, cand_k_dpp, map

rerank: false
rerank_ice: -1
dq: false # use dual query for retrieve
dq_file_path: null # the answer file path for dual query, {'question':...,'answer':...}
pca: false # only one can be set in true in pca and interpolation, use LoRe technique to reduce the dimension of the embeddings
pca_dc: 256 # pca dimension

graphsim: false
graph_path: save_graph/gsm8k

sensim_graphsim: false
alpha: 0.5
n_iter: 3

# parameters needed to initialize the input dataset
dataset_reader:
  _target_: src.dataset_readers.base_dsr.BaseDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: qa
  dataset_split: validation
  dataset_path: null
  ds_size: null

# parameters needed to initialize the index_reader
index_reader:
  _target_: src.dataset_readers.index_dsr.IndexDatasetReader
  #task_name: 'gsm8k'
  task_name: ${task_name}
  model_name: ${model_name}
  field: qa
  dataset_split: null
  dataset_path: null
  ds_size: null

# parameters needed to initialize the bi-encoder model
model_config:
  _target_: src.models.biencoder.BiEncoderConfig
  q_model_name: ${model_name}
  ctx_model_name: ${model_name}
  norm_embed: false
  scale_factor: 0.1  # the factor used to adjust the scale of relevance term and to trade-off diversity and relevance
